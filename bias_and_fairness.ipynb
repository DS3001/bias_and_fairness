{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Fairness\n",
    "`! git clone https://github.com/DS3001/bias_and_fairness`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference, Prediction, and Decision\n",
    "- We're worried about what happens when the prediction tools we build make contact with reality: It's one thing to notice that a group of people are more likely to default on loans, it's another thing to deny them loans or try to avoid advertising to them in order to cut them out of credit markets\n",
    "- People often use two words: **Bias** and **Fairness**, often preceded by the word **Algorithmic** to focus the discussion on machine learning\n",
    "    - Bias means that the data or model produce predictions that systematically fail to represent reality\n",
    "    - Fairness means that the data and model facilitate decision-making that satisfies our moral and ethical standards \n",
    "- My bias is: I generally think that -- outside of a company like Goldman Sachs or Amazon or Google where the explicit goal is to use information to make money at the expense of other agents -- most people are working hard to do the right thing, mistakes are tragic and unavoidable, and the people who are grossly negligent should suffer serious professional consequences. Mostly, using data is better than not, and building a better world is an on-going process. I understand people might disagree, but that's my general initial position without more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Anything Can Go In a Model\n",
    "- First, the bare minimum is to follow the law, and there are many laws that determine what can and cannot be used in a model, and approval processes for machine learning models to be used commercially\n",
    "- For two US laws, here are some (not all) protected and unprotected characteristics:\n",
    "\n",
    "|Attribute| Fair Housing Agreement | Equal Credit Opportunity Act |\n",
    "|:--------|:----------:| :----------: |\n",
    "|Race| Protected | Protected |\n",
    "|National Origin| Protected | Protected |\n",
    "|Religion| Protected | Protected |\n",
    "|Sex| Protected | Protected |\n",
    "|Disability| Protected | Not Protected |\n",
    "|Marital Status| Not Protected | Protected |\n",
    "|Recipient of Public Assistance| Not Protected | Protected |\n",
    "|Age| Not Protected | Protected |\n",
    "\n",
    "- For a predictive model to be used in a clinical setting for decision-making, it must be approved by the FDA (Models are \"Software as a Medical Device,\" or SaMD, and the rules for this are changing rapidly as more and more models are built into medical devices like insulin pumps)\n",
    "- The rules tend to be even stronger in Europe (EU-GDPR) and California (California Consumer Privacy Act, California Privacy Rights Act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Protected Variables:\n",
    "- Immutable characteristics: age (over 40), ancestry, ethnicity, national origin, race\n",
    "- Health data: genetic information, HIV/AIDS status, pregnancy, disability (especially the ADA)\n",
    "- Sexual orientation and gender identity: sex, sexual orientation, gender identity or gender expression\n",
    "- Other: military or veteran status, religion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Even All Research is Allowed!\n",
    "- If an intervention is being performed on human subjects or personally identifiable data is being collected, the research must receive Institutional Review Board approval ( https://www.ecfr.gov/current/title-21/chapter-I/subchapter-A/part-56 , https://www.hhs.gov/ohrp/regulations-and-policy/regulations/45-cfr-46/index.html )\n",
    "- Increasingly, IRB approval is required for projects using data from such interventions, sometimes even when de-identified\n",
    "- This isn't a guarantee the research won't be a disaster, just that it won't harm participants or be negligent with their data\n",
    "- There are three principles from the Belmont Report ( https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/index.html )\n",
    "    1. Respect for persons: Individuals should be treated with dignity and their choices respected; those with diminished autonomy (incarcerated populations, mentally ill, etc.) should receive special protection\n",
    "    2. Beneficience: Researchers should protect subjects by maximizing benefits and minimizing risks\n",
    "    3. Justice: Researchers should distribute the benefits and costs of research fairly\n",
    "- The standard rule-of-thumb for risk is, \"anything more dangerous than common activities of daily life, like drawing blood at a doctor's office or driving a car\"\n",
    "- An important concept is **coercion**: No one should be coerced to participate in research in return for some benefit (e.g. the warden [not the researchers] gives inmate participants who decline to participate extra work, the school principal [not the researchers] gives student participants in a study extra credit)\n",
    "- Obviously, if you don't work at an academic or government institution, the rules are different, but this is a nice \"gold standard\" to keep in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If I Follow the Rules, Aren't I OK?\n",
    "- Not necessarily\n",
    "- There's a legal principle of **disparate impact**: Any practice that disproportionately impacts a protected group might be subject to scrutiny and litigation, especially if the practice is applied inconsistently (e.g. All non-Americans are required to take an ESL test to apply for a job, but it's never actually enforced for Europeans.)\n",
    "- We know that it's very to create a model that facilitates disparate impact because of omitted variables bias: Any person in a protected category that is correlated with variables in the model will receive a prediction that is correlated with their protected category\n",
    "- Humans do, in fact, have to review or audit models scrupulously to try to understand where good intentions or biased data can go wrong: Even well-intentioned modelers can end up unintentionally creating harm\n",
    "- Let's look at some of the known ways harm is created, whether intentional or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Sources of Bias in Statistical Reasoning\n",
    "- Where does the use of statistics go wrong?\n",
    "- There are classical answers that often come up:\n",
    "    1. Sampling Bias: The data don't faithfully reflect the population of interest\n",
    "    2. Simpson's Paradox: Patterns in subsets of the data might not appear when the data are aggregated\n",
    "    3. The Prosecutor's Fallacy: The probability of the data given a condition is not the probability of the condition given the data\n",
    "    4. Omitted Variables Bias: Unobserved variables can still have significant impacts on the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Bias\n",
    "- One thing that makes a data set useful or not is whether it is representative of the population of interest: We typically want samples that faithfully captures the prevalence of different groups and their covariances with other observable variables, so that we know our model will make credible predictions about aggregate behavior \n",
    "- In a recent survey of Virginia, 59.8% of the population was white non-Hispanic, 19.2% of the population was Black, 8% was Hispanic, 7.2% was Asian, 3% was multi-racial, and .3% was indigenous. \n",
    "- In a recent survey of the United States, 60.1% of the population was white non-Hispanic, 12.2% of the population was Black, 18.5% was Hispanic, 5.6% was Asian, 2.8% was multi-racial, and .9% was indigenous.\n",
    "- Imagine using a dataset from Virginia to make predictions about values at the level of the United States: The frequencies for white non-Hispanic and multi-racial are similar, but the rest are not\n",
    "- This means that there Black data will be over-represented while Hispanic/Asian/indigenous will be understated when extrapolating from VA to the US, and vice versa when extrapolating from the US to VA\n",
    "- This is perhaps even worse if our sampling is bad: Imagine our telephone survey in VA only ends up including 12% Black respondents -- That might be representative of the US overall, but not VA specifically\n",
    "- We can attempt to \"fix\" this by re-weighting the data: Treating the 12% of the respondents in our phone survey as 19.2% instead, in order to get more accurate predictions (but why was there an imbalance in the first place?)\n",
    "- This is even more complex if we don't have \"true\" weights for our analysis, and gives you a clue as to why polling for elections is so difficult\n",
    "- A contemporary issue here is facial recognition: Many datasets have disproportionately white faces, so algorithms achieve high accuracy but perform differentially across groups (many algorithms have been proposed to re-weight the data to correct the class imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "- The key tool to understanding how people reason with data is **conditional probability**\n",
    "- It goes like this: \"Given that we know $x$ is true, what is the density/distribution of $y$?\"\n",
    "- We write conditional probabilities as $\\text{pr}[y|x]$. So, $\\text{pr}[\\text{Car Accident}|\\text{Rainy}]$ and $\\text{pr}[\\text{Car Accident}|\\text{Sunny}]$, which are both distinct from $\\text{pr}[\\text{Car Accident}]$ and $\\text{pr}[\\text{Rainy}]$ and $\\text{pr}[\\text{Clear}]$\n",
    "- All regression and classification models are essentially mathematical models for expression $\\mathbb{E}[y|x]$, which is why they are so useful: They are estimates of the conditional expectation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "- We'll start with a silly example to remind ourselves how conditional probability works: Conferences thrown by journal publishers\n",
    "- The attendees are business people, who are outgoing, and librarians, who are shy:\n",
    "\n",
    "\n",
    "|| Outgoing | Shy |\n",
    "|:--------:|:----------:| :-----: |\n",
    "|Librarian| 15 | 40 |\n",
    "|Business| 80 | 60 |\n",
    "\n",
    "- Imagine you're standing around at cocktail hour trying to decide how to talk to shy people: Business or literature?\n",
    "- What's the probability a librarian is shy? 40/(15+40) = .727. Most librarians are shy.\n",
    "- Conditional on meeting a shy person, what is the probability they are a librarian? Well, there are 40 shy librarians, and 40+60=100 shy people, so .4. Conversely, the probability of meeting a shy business person is .6.\n",
    "- Conditional on meeting a business person, what is the probability they are shy? Well, there are 60 shy business people, and 60+80 business people in total, so 60/140 = .429, and outgoing business people must account for 1-.429 = .561.\n",
    "- Do you see that's a bit of a surprise? Given that you've met a shy person, they are more likely to be a business person than a librarian, but conditional on being a business person, they're more likely to be outgoing than shy.\n",
    "- This apparent \"circular reasoning\" is the core of a lot of biased reasoning about probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "- My main motivation of the last slide was to remind you of the definition of conditional probability, and how they're computed:\n",
    "$$\n",
    "\\text{pr}[A|B] = \\dfrac{\\text{pr}[A \\cap B]}{\\text{pr}[B]}\n",
    "$$\n",
    "For example,\n",
    "$$\n",
    "\\text{pr}[\\text{Shy}|\\text{Librarian}] = \\dfrac{\\text{pr}[\\text{Shy and Librarian}]}{\\text{pr}[\\text{Librarian}]} = \\dfrac{40}{15+40}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpson's Paradox\n",
    "- Simpson's Paradox is essentially a composition effect: A property can hold in groupwise comparisons, but be reversed at the aggregate level\n",
    "- An example is: Librarians are systematically more shy than business people, but because there are more business people, a shy person is more likely to be a business person\n",
    "- Reasoning with probabilities rather than counts can be very dangerous and misleading: Probabilities are ratios, and don't \"sum pairwise\"\n",
    "- The classic example is gender bias at UC Berkeley (Bickel, Hammel, and O'Connell, Science 1973). In 1973, at the aggregate level, the admittance rate for men was about 44% and 35% for women. But at the department level, the numbers looked much more comparable except in a few cases.\n",
    "- The take-away of the Berkeley situation is really that if you want decision-making to be fair, you have to ask, \"At what level(s)?\" Both things are true: Admissions was unfair in the aggregate, but was mostly fair at the department level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Prosecutor's Fallacy\n",
    "- The prosecutor's fallacy is reasonably shallow compared to Simpson's paradox: $\\text{pr}[A|B] \\neq \\text{pr}[B|A]$\n",
    "- That's it. It seems simple enough, but people make this mistake all the time\n",
    "- Let's unpack the math:\n",
    "$$\n",
    "\\text{pr}[A|B] = \\dfrac{\\text{pr}[A \\cap B]}{\\text{pr}[B]} \\neq \\dfrac{\\text{pr}[A \\cap B]}{\\text{pr}[A]} = \\text{pr}[B|A]\n",
    "$$\n",
    "- People do this all the time. \"Did you know that noted fascist Hitler was a vegetarian? Obviously, vegetarians are predisposed to authoritarian regimes.\" There are probably plenty of fascist vegetarians, but being a vegetarian has no particular predictive power concerning whether or not one is a fascist. This is the \"ick\" factor of guilt by association at work: Swapping \"behavior|identity\" for \"identity|behavior\"\n",
    "- The issue here is that the prosecutor is supposed to argue $\\text{pr}[\\text{Guilty}|\\text{Evidence}]$ is high, that the person is likely guilty. But that is hard. What is usually much easier is to argue that **if the person were guilty**, then the evidence would be very likely! The prosecutor's fallacy is often deployed as follows:\n",
    "$$\n",
    "\\text{pr}[\\text{Evidence}|\\text{Guilty}] = \\dfrac{\\text{pr}[\\text{Guilty and Evidence}]}{\\text{pr}[\\text{Guilty}]} ``=\" \\dfrac{\\text{pr}[\\text{Guilty and Evidence}]}{\\text{pr}[\\text{Evidence}]} = \\text{pr}[\\text{Guilty}|\\text{Evidence}]\n",
    "$$\n",
    "The fallacy transmutes plausibility of the evidence given guilty into the probability of guilt given the evidence, which is irresponsible \"thinking\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omitted Variables Bias\n",
    "- Again, imagine the true model is $y = b_1 x_1 + b_2 x_2 + e$, but we only have $x_1$ and not $x_2$. When we regress $y$ on $x_1$, our estimate decomposes into two pieces:\n",
    "$$\n",
    "\\underbrace{\\hat{b}_1}_{\\text{Estimated coefficient}} = \\underbrace{b_1}_{\\text{True $x_1$ coefficient}} \\quad + \\quad  \\underbrace{b_2}_{\\text{True $x_2$ coefficient}} \\quad \\underbrace{\\dfrac{\\text{cov}(x_1,x_2)}{s_{x_1}^2}}_{\\text{Regression of $x_2$ on $x_1$}}\n",
    "$$\n",
    "- This is the source of the paradox of protected variables: Not including them can lead to disparate impact and worse model performance\n",
    "- For example: Black men are 1.5 times more likely to be diagnosed with prostate cancer than men of other races or ethnicities and more than twice as likely to die from it. Black people are twice as likely to be diagnosed with stomach cancer, and 2.3 more times likely to die from it.\n",
    "- We don't know exactly why this is, and I can't control for the omitted variables. It's a complex combination of unobserved socio-economic, cultural, genetic, and medical factors that are simply highly correlated with \"Black\", the same way I'll probably get skin cancer but not because I'm \"Irish-American\". \n",
    "- Do you really want me to leave race out of the predictive model for stomach cancer, and ensure that Black people receive a less accurate model than they could otherwise have?\n",
    "- I'm not saying protected variables should always be in models, but there are principled and moral cases for including them: Like accurately predicting outcomes and saving lives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omitted Variables Bias\n",
    "- Just to but an extremely fine point on this, imagine there're two groups, and the true model is\n",
    "$$\n",
    "y_i = a + b m_i + e_i \n",
    "$$\n",
    "where $y_i$ is the outcome for observation $i$, $a$ is the baseline outcome for group 0, $b$ is the impact of membership in group 1 ($m_i =1$ if $i$ is in group 1, and zero otherwise), and $e_i$ is a shock. Let $\\bar{m}$ be the proportion of the population in group 1.\n",
    "- Estimation of this model gives a prediction\n",
    "$$\n",
    "\\hat{y}_i = \\begin{cases} \\bar{y} - \\bar{m} \\frac{\\sum_{j=1}^N (y_j - \\bar{y})m_j}{\\sum_{j=1}^N (m_j - \\bar{m})m_j}, & \\quad i \\text{ in group 0} \\\\\n",
    "\\bar{y} +(1-\\bar{m}) \\frac{\\sum_{j=1}^N (y_j - \\bar{y})m_j}{\\sum_{j=1}^N (m_j - \\bar{m})m_j} , & \\quad i \\text{ in group 1} \n",
    "\\end{cases}\n",
    "$$\n",
    "- If we can't use group membership, we're just estimating the mean: we have no covariates, and $\\hat{y}_i = \\bar{y}$ for everyone\n",
    "- What's the OVB when we omit the group membership variable?\n",
    "$$\n",
    "\\text{bias}_i = \\hat{y}_i - \\bar{y} = \\begin{cases} -\\bar{m} \\frac{\\sum_{j=1}^N (y_j - \\bar{y})m_j}{\\sum_{j=1}^N (m_j - \\bar{m})m_j}, & \\quad i \\text{ in group 0} \\\\\n",
    "(1-\\bar{m}) \\frac{\\sum_{j=1}^N (y_j - \\bar{y})m_j}{\\sum_{j=1}^N (m_j - \\bar{m})m_j} , & \\quad i \\text{ in group 1} \n",
    "\\end{cases}\n",
    "$$\n",
    "- Notice, there is bias for everyone: Throwing away the group membership information leads to biased predictions for both groups\n",
    "- Again, you have moral and legal reasons not to include variables, but it is worth thinking about how OVB shapes the predictions you do make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing an Algorithm\n",
    "- To round out our discussion of bias, let's turn to looking at an algorithm\n",
    "- There are two general approaches:\n",
    "    1. Look at conditional confusion tables or conditional statistics to see whether protected classes are treated systematically differently on average\n",
    "    2. Use empirical cumulative distribution functions to look at continuous variables conditional on protected class status to see if there are systematic differences across the distribution\n",
    "- This is a field that isn't totally settled yet, and lots of people are working on it\n",
    "- We'll look at the justice data that we've been using throughout class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The VPRAI and PSA\n",
    "- We'd like to start with a clear question: Can you provide a credible prediction of which defendants are likely to recidivate before their trial?\n",
    "- I don't think this is a good idea, but let's try to be charitable and sympathetic: Imagine a defendant facing domestic violence charges who has a record of past violence against their previous partners. In such a scenario, we might be credibly worried that the defendant might harm their partner before their trial. Maybe there's a role for this sort of thing in balancing the interests and rights and safety of people in society.\n",
    "- There are two metrics that VA uses to score defendants: VPRAI and PSA. They're based on point totals based on defendant observables, like criminal record and current charges. Let's take a look at their values and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('pretrial2017.csv',low_memory=False) # Pandas downloads and loads the .csv file for you\n",
    "\n",
    "vars = ['Defendant_Sex',\n",
    "'Defendant_Race',\n",
    "'Defendant_Age',\n",
    "'Defendant_AgeGroup',\n",
    "'Defendant_IndigencyStatus',\n",
    "# Criminal Record:\n",
    "'PriorConvs_Fel',\n",
    "'PriorConvs_Misd',\n",
    "'PriorFTAConvs',\n",
    "'PriorVPRAIViolConvs',\n",
    "'PriorVPRAIViolConvs_Fel',\n",
    "'PriorVPRAIViolConvs_Misd',\n",
    "# Contact event:\n",
    "'WhetherDefendantWasReleasedPretrial',\n",
    "'BondTypeAtInitialContact',\n",
    "'BondAmountAtInitialContact',\n",
    "'ContactEvent_WhetherOffensePunishablebyIncaration',\n",
    "'CaseType_MostSerChargeinContactEvent_regardlessofFinalDisp',\n",
    "'CurrentCharge_VPRAIViolent',\n",
    "'CurrentCharge_PSAViolent',\n",
    "'FinalDispositionAllCharges_v1',\n",
    "'ImposedSentenceMostSeriousConvictionInContactEvent',\n",
    "# Judicial monitoring values:\n",
    "'VPRAI_TotalPoints_Opt1',\n",
    "'VPRAI_RiskLevel_Opt2',\n",
    "'VPRAI_TotalPoints_Opt2',\n",
    "'VPRAI_RiskLevel_Opt2',\n",
    "'VPRAI_PTPlacementRiskLevel',\n",
    "'PSA_FTA_TotalPoints',\n",
    "'PSA_FTA_RiskLevelScale',\n",
    "'PSA_NCA_TotalPoints',\n",
    "'PSA_NCA_RiskLevelScale',\n",
    "# Geographic covariates:\n",
    "'UnemploymentRate_October2017_BureauofLaborStatistics',\n",
    "'GiniIndex',\n",
    "'WhiteAlonePercentage_PopulationEstimate_2017_Census',\n",
    "'AfricanAmericanAlonepercentage_PopulationEstimate_2017',\n",
    "'AsianAlonepercentage_PopulationEstimate_2017_Census',\n",
    "'Male_MedianAge_2017Census',\n",
    "'Female_MedianAge_2017Census',\n",
    "# Recidivism variables:\n",
    "'FollowUp_ArrestedforNewOff',\n",
    "'FollowUp_ChargedWithNewFTA',\n",
    "'NewArrest_VCC',\n",
    "'NewArrest_Type',\n",
    "'NewArrest_VCCClassification',\n",
    "'NewArrest_Disposition',\n",
    "'FollowUp_ArrestedForNewOffPunishableByIncarceration',\n",
    "'NewArrestPunishableByIncarceration_Type',\n",
    "'FollowUp_NewFelonyArrest',\n",
    "'NewFelonyArrest_VCC',\n",
    "'NewFelonyArrest_VCCPrefix',\n",
    "'NewFelonyArrest_VCCClassification',\n",
    "'NewFelonyArrest_Type',\n",
    "'NewFelonyArrest_Disposition',\n",
    "'FollowUp_NewVPRAIViolentArrest',\n",
    "'NewVPRAIViolentArrest_VCC',\n",
    "'NewVPRAIViolentArrest_VCCPrefix',\n",
    "'NewVaCode297ViolentOff_VCC',\n",
    "'NewVaCode297ViolentOff_Disposition',]\n",
    "\n",
    "gdf = df.loc[:,vars]\n",
    "gdf['recid'] = 1- (gdf[\"NewArrest_Type\"] == ' ').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewFelonyArrest_Type\n",
       "     21584\n",
       "F     1402\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf['NewFelonyArrest_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vprai_pts</th>\n",
       "      <th>psa_pts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vprai_pts</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.745724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psa_pts</th>\n",
       "      <td>0.745724</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           vprai_pts   psa_pts\n",
       "vprai_pts   1.000000  0.745724\n",
       "psa_pts     0.745724  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf['vprai_pts'] = pd.to_numeric(gdf['VPRAI_TotalPoints_Opt1'], errors='coerce')\n",
    "gdf['psa_pts'] = pd.to_numeric(gdf['PSA_NCA_TotalPoints'], errors='coerce')\n",
    "sns.scatterplot(x='vprai_pts',y='psa_pts',data=gdf)\n",
    "gdf.loc[:,['vprai_pts','psa_pts']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">recid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vprai_pts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>7252.0</td>\n",
       "      <td>0.133756</td>\n",
       "      <td>0.340414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>44.0</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.347142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>4832.0</td>\n",
       "      <td>0.192674</td>\n",
       "      <td>0.394440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>2455.0</td>\n",
       "      <td>0.204073</td>\n",
       "      <td>0.403105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.247715</td>\n",
       "      <td>0.431883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>1621.0</td>\n",
       "      <td>0.258482</td>\n",
       "      <td>0.437936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>1861.0</td>\n",
       "      <td>0.231059</td>\n",
       "      <td>0.421623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>1113.0</td>\n",
       "      <td>0.234501</td>\n",
       "      <td>0.423877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>776.0</td>\n",
       "      <td>0.264175</td>\n",
       "      <td>0.441177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>793.0</td>\n",
       "      <td>0.255990</td>\n",
       "      <td>0.436692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>336.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.433659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>158.0</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>0.443175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.293651</td>\n",
       "      <td>0.457252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.0</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.413155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            recid                                              \n",
       "            count      mean       std  min  25%  50%   75%  max\n",
       "vprai_pts                                                      \n",
       "0.0        7252.0  0.133756  0.340414  0.0  0.0  0.0  0.00  1.0\n",
       "1.0          44.0  0.136364  0.347142  0.0  0.0  0.0  0.00  1.0\n",
       "2.0        4832.0  0.192674  0.394440  0.0  0.0  0.0  0.00  1.0\n",
       "3.0        2455.0  0.204073  0.403105  0.0  0.0  0.0  0.00  1.0\n",
       "4.0        1094.0  0.247715  0.431883  0.0  0.0  0.0  0.00  1.0\n",
       "5.0        1621.0  0.258482  0.437936  0.0  0.0  0.0  1.00  1.0\n",
       "6.0        1861.0  0.231059  0.421623  0.0  0.0  0.0  0.00  1.0\n",
       "7.0        1113.0  0.234501  0.423877  0.0  0.0  0.0  0.00  1.0\n",
       "8.0         776.0  0.264175  0.441177  0.0  0.0  0.0  1.00  1.0\n",
       "9.0         793.0  0.255990  0.436692  0.0  0.0  0.0  1.00  1.0\n",
       "10.0        336.0  0.250000  0.433659  0.0  0.0  0.0  0.25  1.0\n",
       "11.0        158.0  0.265823  0.443175  0.0  0.0  0.0  1.00  1.0\n",
       "12.0        126.0  0.293651  0.457252  0.0  0.0  0.0  1.00  1.0\n",
       "13.0         38.0  0.210526  0.413155  0.0  0.0  0.0  0.00  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf.sort_values('vprai_pts',axis=0)\n",
    "gdf.loc[:,['recid', 'vprai_pts'] ].groupby('vprai_pts').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">recid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psa_pts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>5690.0</td>\n",
       "      <td>0.115466</td>\n",
       "      <td>0.319611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2528.0</td>\n",
       "      <td>0.174842</td>\n",
       "      <td>0.379907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>3599.0</td>\n",
       "      <td>0.207558</td>\n",
       "      <td>0.405615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>1905.0</td>\n",
       "      <td>0.212598</td>\n",
       "      <td>0.409253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>2120.0</td>\n",
       "      <td>0.225472</td>\n",
       "      <td>0.417991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>1921.0</td>\n",
       "      <td>0.235815</td>\n",
       "      <td>0.424617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>1374.0</td>\n",
       "      <td>0.243814</td>\n",
       "      <td>0.429538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>1107.0</td>\n",
       "      <td>0.238482</td>\n",
       "      <td>0.426348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>1027.0</td>\n",
       "      <td>0.249270</td>\n",
       "      <td>0.432801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>834.0</td>\n",
       "      <td>0.268585</td>\n",
       "      <td>0.443490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>283.0</td>\n",
       "      <td>0.261484</td>\n",
       "      <td>0.440221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>107.0</td>\n",
       "      <td>0.271028</td>\n",
       "      <td>0.446582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          recid                                               \n",
       "          count      mean       std  min   25%  50%   75%  max\n",
       "psa_pts                                                       \n",
       "0.0      5690.0  0.115466  0.319611  0.0  0.00  0.0  0.00  1.0\n",
       "1.0      2528.0  0.174842  0.379907  0.0  0.00  0.0  0.00  1.0\n",
       "2.0      3599.0  0.207558  0.405615  0.0  0.00  0.0  0.00  1.0\n",
       "3.0      1905.0  0.212598  0.409253  0.0  0.00  0.0  0.00  1.0\n",
       "4.0      2120.0  0.225472  0.417991  0.0  0.00  0.0  0.00  1.0\n",
       "5.0      1921.0  0.235815  0.424617  0.0  0.00  0.0  0.00  1.0\n",
       "6.0      1374.0  0.243814  0.429538  0.0  0.00  0.0  0.00  1.0\n",
       "7.0      1107.0  0.238482  0.426348  0.0  0.00  0.0  0.00  1.0\n",
       "8.0      1027.0  0.249270  0.432801  0.0  0.00  0.0  0.00  1.0\n",
       "9.0       834.0  0.268585  0.443490  0.0  0.00  0.0  1.00  1.0\n",
       "10.0      283.0  0.261484  0.440221  0.0  0.00  0.0  1.00  1.0\n",
       "11.0      107.0  0.271028  0.446582  0.0  0.00  0.0  1.00  1.0\n",
       "12.0        4.0  0.750000  0.500000  0.0  0.75  1.0  1.00  1.0\n",
       "13.0        2.0  0.500000  0.707107  0.0  0.25  0.5  0.75  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf.sort_values('psa_pts',axis=0)\n",
    "gdf.loc[:,['recid', 'psa_pts'] ].groupby('psa_pts').describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
